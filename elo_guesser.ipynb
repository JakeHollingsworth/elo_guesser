{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2192a4",
   "metadata": {},
   "source": [
    "# Guess the ELO Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cd694",
   "metadata": {},
   "source": [
    "In this notebook, we begin training a transformer to predict the ELO rating of 2 participants in a chess game given the move order. ELO ratings attempt to quantify and rank the ability of chess players; they are determined by the lichess ELO rating system (Glicko 2 rating system: https://lichess.org/page/rating-systems). \n",
    "\n",
    "We use games selected from a single month of the lichess open database (https://database.lichess.org/). Only rapid time control games are included. Data is stored in .pgn format, a standard file format for recording chess games. Each .pgn file contains information for multiple games. For each game in the .pgn file, we extract the ELO of the white and black player, the move order, given in algebraic notation (https://en.wikipedia.org/wiki/Algebraic_notation_(chess)) and the result of the game (\"1-0\" for white win, \"1/2-1/2\" for a draw, and \"0-1\" for black win) which is concatenated at the end of the move history. These are then written to a .csv file for easier processing.\n",
    "\n",
    "We train a RoBERTa model from the Hugging Face library (https://huggingface.co/docs/transformers/index) on the bidirectional (dynamic) masked machine learning task in order to get an informative fixed size representation of the move history. To our knowledge, no existing transformer model exists which has been trained on a large corpus of chess games in algebraic notation, and so transfer learning is not available and training begins with a random initialization of weights. After completing training on the masked language model task, we fine-tune the model to solve the particular problem of predicting player elo ratings from move history. We organize our models in this way to mimic the organization of standard NLP applications, where a large pretraining step is done on the masked machine learning task, which is then fine-tuned for a particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbc216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn # For parsing pgn file\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot\n",
    "import os, sys\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "FORMAT = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "tokenizer_data = 'all_moves.txt'\n",
    "transformer_data = 'mlm_training.txt'\n",
    "model_dir = './inBERTnational_master'\n",
    "\n",
    "num_games = 200000\n",
    "num_test = 10000\n",
    "vocab_size = 2000\n",
    "max_tokens=200\n",
    "train_split = .7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec584fe9",
   "metadata": {},
   "source": [
    "## Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db25d8",
   "metadata": {},
   "source": [
    "Data is originally stored in .pgn format (standard format used for storing chess games). In this cell, we use pychess to parse the pgn file and extract the moves + result as a string and the ELO of the white and black player. The original Lichess pgn files are quite large (256 GB). This is too large for our current resources and is likely more data than needed, given that standard algebraic notation is a fairly constrained \"language.\" We parse games until 1 GB of games are stored in a csv file to accommodate for our computational resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec14cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_game(game):\n",
    "    white_elo = game.headers[\"WhiteElo\"]\n",
    "    black_elo = game.headers[\"BlackElo\"]\n",
    "    result = game.headers[\"Result\"]\n",
    "    board = chess.Board()\n",
    "    move_history = board.variation_san(game.mainline_moves()).split()\n",
    "    # Any strings ending in . are numbers i.e. 1. 2. 3. 4., ... these are just \"grammar\" and don't provide any additional information and so are removed.\n",
    "    move_history = \" \".join(mv for mv in move_history if not mv[-1] == \".\")\n",
    "    move_history += \" \" + result\n",
    "    return move_history, white_elo, black_elo\n",
    "\n",
    "# Downloaded from Lichess database linked above\n",
    "large_pgn_file = '/media/sql/Samsung_T5/lichess_db_standard_rated_2021-03.pgn'\n",
    "small_pgn_file_size = 1 # in GB\n",
    "small_pgn_file = f'/home/sql/Documents/elo_guesser/data_file_{small_pgn_file_size}GB.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "383c5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(large_pgn_file) as large_pgn:\n",
    "    with open(small_pgn_file, 'a') as small_pgn: \n",
    "        game = chess.pgn.read_game(large_pgn)\n",
    "        while game is not None and os.path.getsize(small_pgn_file) < small_pgn_file_size * 1000000000:\n",
    "            if all([head in game.headers for head in ['WhiteElo', 'BlackElo', 'Result','Event']]) and game.headers['Event']=='Rated Rapid game':\n",
    "                move_history, white_elo, black_elo = read_one_game(game)\n",
    "                small_pgn.write(\",\".join([move_history, white_elo, black_elo]) + \"\\n\")\n",
    "            game = chess.pgn.read_game(large_pgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec605dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 20:36:45,773 Number of games total: 3596879\n"
     ]
    }
   ],
   "source": [
    "# Convert CSV to plain text for MLM\n",
    "X = pd.read_csv('data_file_1GB.csv', sep=',', names = ['moves','white_elo', 'black_elo'])\n",
    "\n",
    "logging.info(f'Number of games total: {X.shape[0]}')\n",
    "# Used to train tokenizer\n",
    "if os.path.exists(tokenizer_data):\n",
    "    os.remove(tokenizer_data)\n",
    "with open(tokenizer_data, 'a') as f:\n",
    "    moves_as_str = X['moves']\n",
    "    moves_as_str = moves_as_str.to_csv(sep='\\n', header=False, index=False)\n",
    "    f.write(moves_as_str)\n",
    "\n",
    "# Used to train model\n",
    "if os.path.exists(transformer_data):\n",
    "    os.remove(transformer_data)\n",
    "with open(transformer_data, 'a') as f:\n",
    "    moves_as_str = X['moves'].iloc[:num_games]\n",
    "    moves_as_str = moves_as_str.to_csv(sep='\\n', header=False, index=False)\n",
    "    f.write(moves_as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25a34e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61bb9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./inBERTnational_master/vocab.json', './inBERTnational_master/merges.txt']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a tokenizer for standard algebraic notation from scratch using Hugging Face libary\n",
    "mlm_data_files = [tokenizer_data]\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<s>\", \"</s>\", \"<mask>\", \"<pad>\", \"<unk>\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.enable_truncation(max_length=max_tokens)\n",
    "tokenizer.train(mlm_data_files, trainer)\n",
    "tokenizer.model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9ced2",
   "metadata": {},
   "source": [
    "### Masked Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74323a",
   "metadata": {},
   "source": [
    "We build a transformer to solve the (dynamic) masked language problem for the \"language\" of standard algebraic notation. This section makes large use of the Hugging Face library and in particular borrows code and ideas from the google colab notebook associated with this (https://huggingface.co/blog/how-to-train) blog post. We are unable to use pretrained Hugging Face models as standard algebraic notation is not a language for which a pretrained model exists.\n",
    "\n",
    "It's worth noting that the amount of data here is very large; we have stored nearly 3.6 million games. As such the transformer takes fairly long to train, especially since we don't currently have access to a local gpu. Training is done in a google colab notebook seperately and the trained model will be available here. 12 hour runtime limits are bypassed by frequent checkpointing / restarting the colab kernel. Memory limits are bypassed by splitting the source text into multiple smaller text files and looping over these text files. These details of implementation complicate the code, though, and so are only shown in the colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a58ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./inBERTnational_master/tokenizer.json. We won't load it.\n",
      "Didn't find file ./inBERTnational_master/added_tokens.json. We won't load it.\n",
      "Didn't find file ./inBERTnational_master/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./inBERTnational_master/tokenizer_config.json. We won't load it.\n",
      "loading file ./inBERTnational_master/vocab.json\n",
      "loading file ./inBERTnational_master/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./inBERTnational_master/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./inBERTnational_master\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceMultiTargetRegression\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2000\n",
      "}\n",
      "\n",
      "loading configuration file ./inBERTnational_master/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./inBERTnational_master\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceMultiTargetRegression\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2000\n",
      "}\n",
      "\n",
      "Creating features from dataset file at mlm_training.txt\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 200000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 6:55:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.470100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.334900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.301300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_model/checkpoint-500\n",
      "Configuration saved in ./trained_model/checkpoint-500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./trained_model/checkpoint-1000\n",
      "Configuration saved in ./trained_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./trained_model/checkpoint-1500\n",
      "Configuration saved in ./trained_model/checkpoint-1500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-2000\n",
      "Configuration saved in ./trained_model/checkpoint-2000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-2500\n",
      "Configuration saved in ./trained_model/checkpoint-2500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-3000\n",
      "Configuration saved in ./trained_model/checkpoint-3000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-3500\n",
      "Configuration saved in ./trained_model/checkpoint-3500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-4000\n",
      "Configuration saved in ./trained_model/checkpoint-4000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-4500\n",
      "Configuration saved in ./trained_model/checkpoint-4500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-5000\n",
      "Configuration saved in ./trained_model/checkpoint-5000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-5500\n",
      "Configuration saved in ./trained_model/checkpoint-5500/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./trained_model/checkpoint-6000\n",
      "Configuration saved in ./trained_model/checkpoint-6000/config.json\n",
      "Model weights saved in ./trained_model/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [trained_model/checkpoint-5000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./inBERTnational_master\n",
      "Configuration saved in ./inBERTnational_master/config.json\n",
      "Model weights saved in ./inBERTnational_master/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# First train RoBERTa for Masked Langauge Modelling to get a rich representation of input string.\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=3,\n",
    "    type_vocab_size=1,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_dir, max_len=max_tokens)\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "dataset =  LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=transformer_data,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe49225",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_dir, max_len=max_tokens)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386d171",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc814f98",
   "metadata": {},
   "source": [
    "We fine-tune the standard algebraic notation masked language model to the particular problem of predicting elo ratings of participating players. This is done similarly to as before, with the only 2 differences:\n",
    "\n",
    "The data now also contains elo ratings of the 2 players under the \"targets\" key.\n",
    "\n",
    "The objective is a simple L2 loss of predicted player ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49eacec",
   "metadata": {},
   "source": [
    "Hugging Face provides a RobertaForSequenceClassification wrapper, which can also be used for regression by setting config.num_labels to 1. However, we need to perform multi-target regression, so the provided class must be modified. This is done in the cell below, which is very similar to the RobertaForSequenceClassification class, but tweaks the relevant lines to allow for multi-target regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2581fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, RobertaModel, RobertaForSequenceClassification\n",
    "\n",
    "class RobertaForSequenceMultiTargetRegression(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.n_outputs = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.regressor = RobertaRegressionHead(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        targets = None):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        head_output = self.regressor(sequence_output)\n",
    "\n",
    "        outputs = (head_output,) + outputs[2:]\n",
    "\n",
    "        loss_fct = torch.nn.MSELoss()\n",
    "        loss = loss_fct(head_output.view(-1,self.n_outputs), targets.view(-1,self.n_outputs))\n",
    "        outputs = (loss,) + outputs\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "    \n",
    "class RobertaRegressionHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level Regression tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "# The loss function is changed by redefining the compute_loss() function in the Trainer class.\n",
    "class MultiTargetTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        targets = inputs['targets']\n",
    "        outputs = model(**inputs)\n",
    "        preds = outputs[1]\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(preds.view(-1, self.model.config.num_labels),\n",
    "                        targets.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b979d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./inBERTnational_master/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2000\n",
      "}\n",
      "\n",
      "loading weights file ./inBERTnational_master/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./inBERTnational_master were not used when initializing RobertaForSequenceMultiTargetRegression: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceMultiTargetRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceMultiTargetRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceMultiTargetRegression were not initialized from the model checkpoint at ./inBERTnational_master and are newly initialized: ['regressor.dense.bias', 'roberta.pooler.dense.weight', 'regressor.dense.weight', 'regressor.out_proj.bias', 'roberta.pooler.dense.bias', 'regressor.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd31c4a63024cebbfcc92d8687c6a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cde10e8fdc4ce6aa391778cf139cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceMultiTargetRegression.forward` and have been ignored: moves.\n",
      "***** Running training *****\n",
      "  Num examples = 140000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2187' max='2187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2187/2187 9:29:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2221194.752000</td>\n",
       "      <td>1878776.282972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1474099.200000</td>\n",
       "      <td>1079000.315457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>781082.496000</td>\n",
       "      <td>538470.035470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>419089.664000</td>\n",
       "      <td>327692.268279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceMultiTargetRegression.forward` and have been ignored: moves.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./finetuned_model/checkpoint-500\n",
      "Configuration saved in ./finetuned_model/checkpoint-500/config.json\n",
      "Model weights saved in ./finetuned_model/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceMultiTargetRegression.forward` and have been ignored: moves.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./finetuned_model/checkpoint-1000\n",
      "Configuration saved in ./finetuned_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./finetuned_model/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceMultiTargetRegression.forward` and have been ignored: moves.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./finetuned_model/checkpoint-1500\n",
      "Configuration saved in ./finetuned_model/checkpoint-1500/config.json\n",
      "Model weights saved in ./finetuned_model/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceMultiTargetRegression.forward` and have been ignored: moves.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./finetuned_model/checkpoint-2000\n",
      "Configuration saved in ./finetuned_model/checkpoint-2000/config.json\n",
      "Model weights saved in ./finetuned_model/checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./finetuned_model/checkpoint-2000 (score: 327692.2682793193).\n",
      "Saving model checkpoint to ./inBERTnational_master\n",
      "Configuration saved in ./inBERTnational_master/config.json\n",
      "Model weights saved in ./inBERTnational_master/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    truth = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    L2 = sklearn.metrics.mean_squared_error(truth, preds)\n",
    "    return { \"loss\": L2\n",
    "    }\n",
    "\n",
    "model = RobertaForSequenceMultiTargetRegression.from_pretrained(model_dir)\n",
    "\n",
    "data_dict = {'moves': X['moves'].iloc[:num_games], 'targets': X[['white_elo', 'black_elo']].iloc[:num_games].values}\n",
    "all_data =  Dataset.from_dict(data_dict)\n",
    "train_data = Dataset.from_dict(all_data[:int(train_split * num_games)])\n",
    "test_data = Dataset.from_dict(all_data[int(train_split * num_games):])\n",
    "\n",
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['moves'], padding = True, truncation=True)\n",
    "\n",
    "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
    "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './finetuned_model',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size = 32,\n",
    "    gradient_accumulation_steps = 2,    \n",
    "    per_device_eval_batch_size= 32,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy = \"steps\",\n",
    "    dataloader_num_workers = 8,\n",
    "    label_names=['targets'],\n",
    "    run_name = 'roberta-regression'\n",
    ")\n",
    "\n",
    "trainer = MultiTargetTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('./finetuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9bfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa3ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
